{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from tpot.config.classifier_nn import classifier_config_nn\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tpot.config import classifier_config_dict_light\n",
    "from tpot.config import classifier_config_dict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_config = classifier_config_dict_light\n",
    "personal_config = classifier_config_dict\n",
    "personal_config['tpot.builtins.SimpleAutoencoder'] = {\n",
    "    'encoding_dim': [10],\n",
    "    'activation': ['relu'],\n",
    "    'optimizer': ['adadelta'],\n",
    "    'loss':['binary_crossentropy'],\n",
    "    'epochs':[100],\n",
    "    'batch_size':[200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding function\n",
    "\n",
    "#function that takes in a features pandas dataframe and turns it into a numpy array \n",
    "#with only the categorical variables one-hot encoded (and leaving out one of the features \n",
    "#of every one-hot encoded variable as baseline)\n",
    "#@param:  features is a pandas df of features; threshold number is the number of unique values \n",
    "#a variable should have in order to be considered categorical\n",
    "#@return:  a numpy matrix containing the original dataset but with the categorical variables\n",
    "#one hot encoded and one of the one hot encoded features per cat variable left out; \n",
    "#for example an original feature set of 10 categorical variables with three categories each will \n",
    "#be transoformed into a numpy array with 20 dichotomous variables\n",
    "\n",
    "def one_hot_encode(features, cat_threshold_number = 5):\n",
    "    num_unique_vals_dict = {}\n",
    "    feature_names = list(features)\n",
    "    for feature in feature_names:\n",
    "        label_encoder = LabelEncoder()\n",
    "        features.loc[:, feature] = label_encoder.fit_transform(features.loc[:, feature])\n",
    "        num_unique_vals_dict[feature] = len(label_encoder.classes_)\n",
    "\n",
    "    features_to_onehot = []\n",
    "    for feature in num_unique_vals_dict:\n",
    "        if num_unique_vals_dict[feature] <= cat_threshold_number and num_unique_vals_dict[feature] > 1:\n",
    "            features_to_onehot = features_to_onehot + [feature]\n",
    "\n",
    "    #create index array listing indices in orginal feature names array that are present in features_to_onehot\n",
    "    indices_to_onehot = np.nonzero(np.in1d(feature_names, features_to_onehot))\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(categorical_features = indices_to_onehot, sparse = False)\n",
    "    features = onehot_encoder.fit_transform(features)\n",
    "\n",
    "    idx_to_delete = np.cumsum([0] + list(num_unique_vals_dict.values()))\n",
    "\n",
    "    idx_to_keep = [i for i in range(features.shape[1]) if i not in idx_to_delete]\n",
    "\n",
    "    features = features[:, idx_to_keep]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "data = pd.read_table(\"sample_data.txt\")\n",
    "data = data.iloc[:, :11]\n",
    "dv = data.iloc[:, 10:11]\n",
    "features = data.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode data, then split it into training and validation\n",
    "X = one_hot_encode(features)\n",
    "y = dv\n",
    "y = pd.np.array(y).ravel()\n",
    "#split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_val = x_val.reshape((len(x_val)), np.prod(x_val.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot.builtins import SimpleAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SimpleAutoencoder(encoding_dim=10, \n",
    "                        activation='relu', \n",
    "                        optimizer='adadelta', \n",
    "                        loss='binary_crossentropy', \n",
    "                        epochs=100, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleAutoencoder(activation='relu', batch_size=200, encoding_dim=10,\n",
       "         epochs=100, loss='binary_crossentropy', optimizer='adadelta',\n",
       "         random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.transform(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=10, config_dict=personal_config,\n",
    "                        population_size=10, verbosity=3,\n",
    "                        template = 'SimpleAutoencoder-Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c998d7acef4afeb17a754d25d3c82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=110), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current Pareto front scores:\n",
      "-2\t0.5508027947148135\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-2\t0.5508027947148135\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-2\t0.5537545355927131\tRandomForestClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=14, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-2\t0.5658621678574546\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-2\t0.5672324260069742\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=7, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-2\t0.5673548714239994\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-2\t0.5673548714239994\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-2\t0.5673548714239994\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-2\t0.5673548714239994\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-2\t0.5673548714239994\tXGBClassifier(SimpleAutoencoder(input_matrix, SimpleAutoencoder__activation=relu, SimpleAutoencoder__batch_size=200, SimpleAutoencoder__encoding_dim=10, SimpleAutoencoder__epochs=100, SimpleAutoencoder__loss=binary_crossentropy, SimpleAutoencoder__optimizer=adadelta), XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.naive_bayes.GaussianNB': {}, 'sklearn.naive_bayes.BernoulliNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.naive_bayes.MultinomialNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.tree.DecisionT..., 'optimizer': ['adadelta'], 'loss': ['binary_crossentropy'], 'epochs': [100], 'batch_size': [200]}},\n",
       "        crossover_rate=0.1, cv=5, disable_update_check=False,\n",
       "        early_stop=None, generations=10, max_eval_time_mins=5,\n",
       "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,\n",
       "        offspring_size=None, periodic_checkpoint_folder=None,\n",
       "        population_size=10, random_state=None, scoring=None, subsample=1.0,\n",
       "        template='SimpleAutoencoder-Classifier', use_dask=False,\n",
       "        verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('simpleautoencoder', SimpleAutoencoder(activation='relu', batch_size=200, encoding_dim=10,\n",
       "         epochs=100, loss='binary_crossentropy', optimizer='adadelta',\n",
       "         random_state=42)), ('xgbclassifier', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsampl...=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9500000000000001))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monicai\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monicai\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5272727272727272"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569067cef5aa49cf88d93fde37bf7d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=110), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=10,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=10,\n",
       "        random_state=None, scoring=None, subsample=1.0,\n",
       "        template='RandomTree', use_dask=False, verbosity=3,\n",
       "        warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare with regular TPOT\n",
    "tpot_reg = TPOTClassifier(generations=10, population_size=10, verbosity=3)\n",
    "tpot_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850746268656716"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot_reg.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
